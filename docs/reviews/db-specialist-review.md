# CRStats -- Database Specialist Review

> **Generated by:** @data-engineer (Dara) -- Brownfield Discovery Phase 5
> **Date:** 2026-02-27
> **Reviewing:** `docs/prd/technical-debt-DRAFT.md` (Phase 4 -- @architect)
> **Cross-references:** `docs/architecture/DB-AUDIT.md`, `docs/architecture/SCHEMA.md` (Phase 2 -- @data-engineer)
> **Code verified against:** `shared/schema.ts`, `server/storage.ts`, `server/db.ts`, `server/routes.ts`, `scripts/supabase/rls-and-triggers.sql`

---

## 1. Review Summary

### Items Reviewed

The following TD items from the DRAFT are database-related and fall within @data-engineer scope:

| TD-ID | Title | DRAFT Severity |
|-------|-------|:--------------:|
| TD-012 | Notification settings duplication (`user_settings` vs `notification_preferences`) | HIGH |
| TD-013 | No automatic `updated_at` trigger | HIGH |
| TD-014 | Missing indexes on time-filtered count queries | HIGH |
| TD-015 | `subscriptions.user_id` not unique | HIGH |
| TD-019 | No database connection pool configuration | HIGH |
| TD-022 | No database migration versioning | HIGH |
| TD-025 | `bootstrapUserData()` code duplication | MEDIUM |
| TD-026 | No enum/CHECK constraints at DB level | MEDIUM |
| TD-027 | `coach_messages` and `deck_suggestions_usage` grow unbounded | MEDIUM |
| TD-032 | N+1 queries in `/api/auth/user` | MEDIUM |
| TD-033 | `favorite_players` data staleness | MEDIUM |
| TD-036 | Timestamp columns without timezone | MEDIUM |
| TD-041 | `profiles.clash_tag` legacy column | LOW |
| TD-048 | Index naming convention inconsistency | LOW |
| TD-049 | `create-stripe-prices.ts` uses dead Replit connector | LOW |

**Total: 15 items reviewed** (of 50 total in DRAFT).

### Overall Assessment: PARTIALLY AGREE

The DRAFT accurately captures the database debt items identified in my Phase 2 DB Audit. The descriptions are technically correct, dependencies are well-mapped, and the remediation roadmap is sensible. However, I have the following concerns:

1. **Two severity downgrades from my original audit were made without sufficient justification** -- TD-012 (notification duplication) and TD-013 (updated_at trigger) were both CRITICAL in my audit but reduced to HIGH in the DRAFT.
2. **One item is missing from the DRAFT entirely** -- the `runAsUser` transaction overhead per-query pattern, which I flagged in DB-AUDIT Section 7.2.
3. **Two effort estimates are too optimistic** -- TD-012 and TD-015 underestimate the data migration complexity.
4. **The remediation roadmap places database integrity fixes too late** (Phase 4, Week 4-5), while database items are arguably more foundational than i18n fixes (Phase 3).

### Missing Items Not in DRAFT

| Missing Item | My Audit Reference | Proposed Severity |
|-------------|-------------------|:-----------------:|
| `runAsUser` per-query transaction overhead | DB-AUDIT Section 7.2 | MEDIUM |
| `push_analyses` lacking pruning (separate from TD-027 which groups it vaguely) | DB-AUDIT Section 5.3 | MEDIUM |
| `battle_history.created_at` nullable when it should be NOT NULL | DB-AUDIT Section 3.3 | LOW |

---

## 2. Item-by-Item Review

---

### TD-012: Notification Settings Duplication

- **DRAFT Severity:** HIGH (CRITICAL per DB Audit)
- **Severity Assessment:** **UPGRADE back to CRITICAL.** The DRAFT itself acknowledges this was CRITICAL in my audit but downgraded it. My justification stands: the `isNotificationAllowed()` function (verified at `server/routes.ts:155-173`) implements a two-table fallback chain where `notification_preferences` is checked first, then `user_settings` is checked as a fallback. Any code that writes to one table but not the other creates a silent inconsistency. The `bootstrapUserData()` function (verified at `server/storage.ts:272-443`) and the `handle_new_user()` trigger (verified at `rls-and-triggers.sql:47-116`) both write to BOTH tables, but there is no guarantee that subsequent updates will maintain parity. The `settingsUpdateInputSchema` (at `schema.ts:460-474`) even has a nested `notificationPreferences` object INSIDE the settings update, which means a single API call could update either or both tables. This is not just a "bug waiting to happen" -- it is an active consistency hazard.
- **Technical Accuracy:** Correct. The DRAFT accurately describes the dual-table structure and fallback chain. One minor addition: the DRAFT should note that `settingsUpdateInputSchema` accepts BOTH `notificationsTraining`/`notificationsBilling`/`notificationsSystem` (user_settings columns) AND a nested `notificationPreferences` object, making it possible for a single settings update to touch both tables with conflicting values.
- **Effort Estimate:** **Re-estimate: L to XL (2-4 days).** The DRAFT says L (1-3 days). The actual remediation requires: (1) data audit to identify inconsistencies in production, (2) migration to consolidate data (deciding which table "wins" per row), (3) schema migration to remove redundant columns, (4) update `isNotificationAllowed()` to use a single source, (5) update `settingsUpdateInputSchema` to remove the dual-update path, (6) update `bootstrapUserData()` (both branches), (7) update `handle_new_user()` trigger in SQL. That is 7 change points across 4 files. L is optimistic; XL is safer.
- **Remediation Review:** The DRAFT's suggested approach is correct. My specific recommendation: keep `notification_preferences` as the canonical table (it is purpose-built, has NOT NULL constraints, and is cleaner). Remove `notifications_training`, `notifications_billing`, `notifications_system` from `user_settings`. Migrate data by preferring `notification_preferences` values where they exist, falling back to `user_settings` values.
- **Dependencies:** Agree with DRAFT -- TD-022 (versioned migrations) should come first. Add dependency on TD-025 (bootstrap refactor) since the bootstrap code touches both tables.
- **Priority Adjustment:** Should be in Phase 2 or early Phase 4, not late Phase 4. This is a data integrity issue that gets worse with every new user.

---

### TD-013: No Automatic `updated_at` Trigger

- **DRAFT Severity:** HIGH (CRITICAL per DB Audit)
- **Severity Assessment:** **UPGRADE back to CRITICAL.** I verified `server/storage.ts` has 14 instances of `updatedAt: new Date()` being set manually in application code. The `rls-and-triggers.sql` script has zero `updated_at` trigger functions. The risk is real: any new update path added by a developer who forgets to set `updatedAt` will silently leave stale timestamps. Furthermore, the clock skew issue is not theoretical -- the Vercel serverless environment's system clock and the Supabase PostgreSQL `now()` can differ by seconds, which matters for ordering and caching logic.
- **Technical Accuracy:** Correct. No trigger exists. The DRAFT accurately describes the issue.
- **Effort Estimate:** Agree -- S (< 2h). The trigger function is ~10 lines of SQL, and applying it to 8 tables with `updated_at` columns is straightforward.
- **Remediation Review:** Correct approach. Here is the exact SQL:

```sql
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS trigger AS $$
BEGIN
  NEW.updated_at = now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

Apply to: `users`, `profiles`, `subscriptions`, `goals`, `user_settings`, `notification_preferences`, `player_sync_state`, `training_plans`, `training_drills`. Note: after applying the trigger, the 14 manual `updatedAt: new Date()` calls in `storage.ts` become redundant but harmless (the trigger will overwrite them). They can be removed in a follow-up cleanup.
- **Dependencies:** None. This is the highest-priority database quick win.
- **Priority Adjustment:** Should remain in Phase 1. No objection to its placement.

---

### TD-014: Missing Indexes on Time-Filtered Count Queries

- **DRAFT Severity:** HIGH
- **Severity Assessment:** **Agree -- HIGH.** Verified: `countCoachMessagesToday()` (storage.ts:910-928) filters on `(userId, role='user', createdAt >= todayStart)` but the only index is `IDX_coach_messages_user_id` on `(user_id)` alone. Similarly, `countPushAnalysesToday()` (storage.ts:968-980) filters on `(userId, createdAt >= todayStart)` with only `IDX_push_analyses_user_id` on `(user_id)`. These are rate-limiting queries executed on every coach chat and push analysis request.
- **Technical Accuracy:** Correct. The DRAFT accurately identifies both queries and the missing composite indexes.
- **Effort Estimate:** Agree -- S (< 2h). Two `CREATE INDEX` statements.
- **Remediation Review:** The DRAFT's proposed indexes are correct. One refinement: for `coach_messages`, the index should be `(user_id, role, created_at)` with `role` in the middle because the query always filters `role = 'user'` (a constant), making the btree scan most efficient with the equality predicate columns first. The DRAFT has this right.
- **Dependencies:** None. Standalone quick win.
- **Priority Adjustment:** Agree with Phase 1 placement.

---

### TD-015: `subscriptions.user_id` Not Unique

- **DRAFT Severity:** HIGH
- **Severity Assessment:** **Agree -- HIGH.** Verified at `schema.ts:63-78`: `userId` on the subscriptions table has no unique constraint. The `createSubscription()` method at `storage.ts:506-531` does a SELECT-then-INSERT/UPDATE pattern that is vulnerable to race conditions. If two Stripe webhook events for the same user fire concurrently, both can proceed past the SELECT and both will INSERT, creating orphan rows.
- **Technical Accuracy:** Correct. The DRAFT accurately identifies the issue and the fragile `ORDER BY created_at DESC LIMIT 1` pattern.
- **Effort Estimate:** **Re-estimate: M to L (1-2 days).** The DRAFT says M (2-8h). The actual steps are: (1) identify and clean up existing orphan subscription rows in production (requires a careful audit query), (2) add the unique constraint (which will fail if orphans exist), (3) rewrite `createSubscription()` to use `ON CONFLICT (user_id) DO UPDATE`, (4) verify the `bootstrapUserData()` subscription creation path (which also checks-then-inserts). The data cleanup alone can be tricky -- you need to decide which orphan row to keep (likely the one with `plan = 'pro'` if any, otherwise the most recent). This is closer to 1 day than "2-8 hours."
- **Remediation Review:** The DRAFT's approach is correct. My specific recommendation:

```sql
-- Step 1: Cleanup (run manually, verify results first)
DELETE FROM subscriptions s
WHERE s.id NOT IN (
  SELECT DISTINCT ON (user_id) id
  FROM subscriptions
  ORDER BY user_id,
    CASE WHEN plan = 'pro' THEN 0 ELSE 1 END,
    created_at DESC
);

-- Step 2: Add constraint
ALTER TABLE subscriptions ADD CONSTRAINT subscriptions_user_id_unique UNIQUE (user_id);
```

After the constraint is in place, refactor `createSubscription()` to use `ON CONFLICT (user_id) DO UPDATE`. Also update `bootstrapUserData()` to use `ON CONFLICT (user_id) DO NOTHING` instead of the SELECT-then-INSERT pattern.
- **Dependencies:** Agree -- data cleanup migration is required. TD-022 (versioned migrations) is a soft dependency but not a hard blocker (the cleanup can be a manual SQL script).
- **Priority Adjustment:** I recommend elevating this to early Phase 2 rather than Phase 4. The race condition is real for any project with Stripe webhooks, and the fix is not complex once you commit to the unique constraint.

---

### TD-019: No Database Connection Pool Configuration

- **DRAFT Severity:** HIGH
- **Severity Assessment:** **Agree -- HIGH.** Verified at `server/db.ts:13`: `new Pool({ connectionString: process.env.DATABASE_URL })` with zero configuration. The pg default is `max: 10` connections, but on Vercel serverless, each cold start creates a new Pool instance. With concurrent serverless invocations, you can easily exceed Supabase's connection limits (60 direct connections on free, 200 on Pro).
- **Technical Accuracy:** Correct. The DRAFT accurately describes the issue and the serverless context.
- **Effort Estimate:** Agree -- S (< 2h).
- **Remediation Review:** The DRAFT's recommendation is correct. However, I would add a stronger recommendation: **use Supabase's connection pooler (PgBouncer) URL** as the `DATABASE_URL` for the application. The direct connection URL should only be used for migrations (which is already partially reflected in `drizzle.config.ts` using `DATABASE_MIGRATIONS_URL`). Specific config:

```typescript
export const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 3,                      // Low for serverless
  connectionTimeoutMillis: 5000,
  idleTimeoutMillis: 10000,    // Short for serverless (connections reclaimed quickly)
});
```

Additionally, add `process.env.DATABASE_MIGRATIONS_URL` for Drizzle migrations and keep it as the direct (non-pooled) connection.
- **Dependencies:** None.
- **Priority Adjustment:** Agree with Phase 1 placement.

---

### TD-022: No Database Migration Versioning

- **DRAFT Severity:** HIGH
- **Severity Assessment:** **Agree -- HIGH.** Verified: `drizzle.config.ts` exists with `out: "./migrations"` configured, but the `migrations/` directory does not exist. The project uses `drizzle-kit push` exclusively. The SQL scripts (`rls-and-triggers.sql`, `decks-migrations.sql`) serve as ad-hoc migrations for Supabase-specific features.
- **Technical Accuracy:** Correct. One nuance the DRAFT missed: `drizzle.config.ts` already has the `out: "./migrations"` path configured, meaning the infrastructure for versioned migrations is partially set up -- only the workflow needs to change from `push` to `generate` + `migrate`. This reduces the effort slightly.
- **Effort Estimate:** Agree -- M (2-8h). The config is already there; it is primarily a workflow change plus generating the initial baseline migration.
- **Remediation Review:** Correct approach. My specific recommendation: (1) run `drizzle-kit generate` to create the baseline migration, (2) verify the generated SQL matches the current production schema, (3) change `package.json` scripts from `db:push` to `db:migrate`, (4) keep `rls-and-triggers.sql` and `decks-migrations.sql` as a separate Supabase-specific apply step (these cannot be managed by Drizzle).
- **Dependencies:** None. Unblocks TD-012, TD-015, TD-026, TD-036, TD-041.
- **Priority Adjustment:** Agree with Phase 2 placement. Should be done early in Phase 2.

---

### TD-025: `bootstrapUserData()` Code Duplication

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Agree -- MEDIUM.** Verified at `storage.ts:272-443`: two nearly identical blocks (~85 lines each) for the `!this.auth` and `this.auth` branches. The only difference is the first block uses `db.transaction()` directly while the second uses `this.runAsUser()`.
- **Technical Accuracy:** Correct. The DRAFT accurately describes the duplication.
- **Effort Estimate:** Agree -- S (< 2h).
- **Remediation Review:** The DRAFT's approach is correct. Extract the inner logic into a private method like `_bootstrapInTransaction(tx, userId)` and call it from both branches. The `!this.auth` branch wraps in `db.transaction()`, the `this.auth` branch wraps in `this.runAsUser()`.
- **Dependencies:** None. Should be done before TD-012 (notification consolidation) to reduce the number of places that need updating.
- **Priority Adjustment:** Agree with Phase 2 placement.

---

### TD-026: No Enum/CHECK Constraints at DB Level

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Agree -- MEDIUM.** The DRAFT accurately reflects my audit finding. All enum validation is Zod-only. I verified no CHECK constraints exist in `schema.ts` or `rls-and-triggers.sql`.
- **Technical Accuracy:** Correct. The DRAFT lists the right columns.
- **Effort Estimate:** Agree -- S (< 2h).
- **Remediation Review:** Correct approach. The CHECK constraints should be added via a versioned migration. Priority order: `subscriptions.plan`, `subscriptions.status` (billing-critical), then `training_drills.status`, `training_plans.status`, `goals.type`. For Drizzle ORM, CHECK constraints can be added via `sql` in the schema or via raw SQL migration.

```sql
ALTER TABLE subscriptions ADD CONSTRAINT chk_subscriptions_plan
  CHECK (plan IN ('free', 'pro'));
ALTER TABLE subscriptions ADD CONSTRAINT chk_subscriptions_status
  CHECK (status IN ('inactive', 'active', 'canceled', 'past_due'));
ALTER TABLE goals ADD CONSTRAINT chk_goals_type
  CHECK (type IN ('trophies', 'streak', 'winrate', 'custom'));
ALTER TABLE training_plans ADD CONSTRAINT chk_training_plans_status
  CHECK (status IN ('active', 'archived', 'completed'));
ALTER TABLE training_drills ADD CONSTRAINT chk_training_drills_status
  CHECK (status IN ('pending', 'in_progress', 'completed', 'skipped'));
```

- **Dependencies:** TD-022 (versioned migrations) recommended first.
- **Priority Adjustment:** Agree with Phase 4 placement.

---

### TD-027: `coach_messages` and `deck_suggestions_usage` Grow Unbounded

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Agree -- MEDIUM.** The DRAFT correctly groups multiple unbounded tables. However, the description could be more precise about the different growth characteristics:
  - `coach_messages`: ~2-4 rows per coach interaction (user message + assistant reply). Heavy users could generate 50-100 rows/day. This is the highest-volume unbounded table.
  - `deck_suggestions_usage`: 1 row per deck suggestion. Free users are rate-limited to 2/day, so growth is bounded by user count, not activity.
  - `push_analyses`: 1 row per push analysis. Much lower volume than coach_messages.
  - `notifications`: Event-driven. Moderate growth.
- **Technical Accuracy:** Mostly correct. The DRAFT vaguely includes `push_analyses` in the group but does not give it its own pruning strategy.
- **Effort Estimate:** Agree -- M (2-8h).
- **Remediation Review:** The DRAFT's approach is sound. My specific recommendations per table:
  - `coach_messages`: 90-day retention for free, 365-day for pro. Implement as a daily cron job.
  - `deck_suggestions_usage`: 30-day retention. Only today's rows matter for rate limiting.
  - `push_analyses`: 180-day retention for free, 365-day for pro. Keep `result_json` for historical analysis within window.
  - `notifications`: 30-day retention for read notifications. Unread notifications should be kept longer (90 days) to avoid silently dropping them.
- **Dependencies:** None.
- **Priority Adjustment:** Agree with Phase 4 placement.

---

### TD-032: N+1 Queries in `/api/auth/user`

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Agree -- MEDIUM.** The concern is valid but the actual impact is mitigated by the fact that each query is simple (single-row lookup by PK or indexed FK) and they all execute within a single `runAsUser` transaction when auth is present. The overhead is primarily from the 4 SET commands per `runAsUser` call (not per query), plus the sequential round trips.
- **Technical Accuracy:** **Correction needed.** The DRAFT says "5 separate queries per authentication request." Looking at the actual auth flow, `bootstrapUserData()` already includes fetching profile, settings, subscription, and notification preferences at the end (lines 333-345 or 419-431 in storage.ts). So if `bootstrapUserData()` is called, only 2 runAsUser calls are needed (getUser + bootstrapUserData), not 5. The 5-query scenario only occurs when the DRAFT describes the case where bootstrapUserData is NOT called and the data is fetched individually. The DRAFT should clarify this distinction.
- **Effort Estimate:** Agree -- M (2-8h).
- **Remediation Review:** The DRAFT's suggestion to consolidate into a single SQL join is correct. A more pragmatic approach: since `bootstrapUserData()` already returns all 4 entities, the route handler should use the bootstrap result directly instead of making separate calls. Only add a JOIN-based query if profiling shows the bootstrap path is too slow.
- **Dependencies:** None.
- **Priority Adjustment:** Agree with Phase 4 placement.

---

### TD-033: `favorite_players` Data Staleness

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Agree -- MEDIUM.** Verified at `schema.ts:122-137`: `trophies` and `clan` are stored but never refreshed after initial save.
- **Technical Accuracy:** Correct.
- **Effort Estimate:** Agree -- M (2-8h).
- **Remediation Review:** The DRAFT offers two approaches. I recommend the simpler one: refresh cached data during player sync. When the `/api/player/sync` endpoint fetches player data from the Clash Royale API, also update any matching `favorite_players` rows for that user. This piggybacks on an existing API call and avoids additional external requests.
- **Dependencies:** None.
- **Priority Adjustment:** Agree with Phase 4 placement.

---

### TD-036: Timestamp Columns Without Timezone

- **DRAFT Severity:** MEDIUM
- **Severity Assessment:** **Downgrade to LOW.** The DRAFT says MEDIUM. In practice, CRStats uses UTC everywhere, both in application code (`new Date()` produces UTC in Node.js) and in PostgreSQL (`now()` returns UTC when the server timezone is UTC, which is the Supabase default). The risk of timezone-related bugs is theoretical, not practical. PostgreSQL internally stores `timestamp` and `timestamptz` identically when the input is UTC -- the difference only matters at input/output conversion. Since all I/O goes through Drizzle (which handles timestamps as JavaScript `Date` objects in UTC), the practical risk is near-zero.
- **Technical Accuracy:** The DRAFT's description is technically correct but overstates the risk.
- **Effort Estimate:** Agree -- S (< 2h) for the schema change, but the migration requires updating all 16 tables' timestamp columns, which is more like M (2-8h) when including testing. The DRAFT acknowledges this nuance.
- **Remediation Review:** Correct approach. Use `timestamptz` in Drizzle:

```typescript
createdAt: timestamp("created_at", { withTimezone: true }).defaultNow(),
```

- **Dependencies:** TD-022 (versioned migrations).
- **Priority Adjustment:** Move to Phase 5 (LOW priority). Not worth the migration risk in Phase 4.

---

### TD-041: `profiles.clash_tag` Legacy Column

- **DRAFT Severity:** LOW
- **Severity Assessment:** **Agree -- LOW.** Verified: `buildCanonicalProfileData()` at `storage.ts:89-113` keeps both columns in sync. The redundancy is confusing but not harmful.
- **Technical Accuracy:** Correct.
- **Effort Estimate:** Agree -- S (< 2h).
- **Remediation Review:** Correct approach. The deprecation plan is sound.
- **Dependencies:** TD-022 (versioned migrations).
- **Priority Adjustment:** Agree with Phase 5 placement.

---

### TD-048: Index Naming Convention Inconsistency

- **DRAFT Severity:** LOW
- **Severity Assessment:** **Agree -- LOW.** Cosmetic only.
- **Technical Accuracy:** Correct.
- **Effort Estimate:** Agree -- S (< 2h).
- **Remediation Review:** Correct. Rename indexes during a versioned migration.
- **Dependencies:** TD-022.
- **Priority Adjustment:** Agree with Phase 5 placement.

---

### TD-049: `create-stripe-prices.ts` Uses Dead Replit Connector

- **DRAFT Severity:** LOW
- **Severity Assessment:** **Agree -- LOW.** Dead code.
- **Technical Accuracy:** Correct.
- **Effort Estimate:** Agree -- S (< 2h). Just delete the file.
- **Remediation Review:** Correct.
- **Dependencies:** None.
- **Priority Adjustment:** Agree with Phase 1 placement (quick win).

---

## 3. Missing Database Debt Items

The DRAFT missed the following items that I identified in my Phase 2 audit or during this code review:

---

### TD-NEW-1: `runAsUser` Per-Query Transaction Overhead

- **Suggested ID:** TD-051
- **Severity:** MEDIUM
- **Source:** DB-AUDIT.md Section 7.2
- **Description:** Every single database operation in `DatabaseStorage` (including simple SELECTs like `getUser`, `getProfile`, `getUserSettings`) is wrapped in a `runAsUser()` call that opens a new transaction with 4 `SET` commands (`set_config` x3 + `SET LOCAL ROLE`). For a sequence of 3 reads, that is 3 transactions with 12 SET commands, instead of 1 transaction with 4 SET commands and 3 queries.
- **Impact:** Increased latency and connection pool pressure. Each `runAsUser` acquires a connection from the pool, starts a transaction, executes 4 SET commands, runs the query, commits, and returns the connection. For the auth endpoint (which calls multiple storage methods), this creates unnecessary round trips.
- **Remediation:** Introduce a "session" concept that reuses a single transaction for multiple storage operations. For example, a `withUserSession(auth, async (session) => { ... })` wrapper that opens one transaction, sets RLS context once, and allows multiple queries within it. Individual storage methods should accept an optional session/transaction parameter.
- **Effort:** L (1-3 days) -- requires refactoring the `runAsUser` pattern across all 60+ methods.
- **Dependencies:** Should be done alongside or after TD-001 (routes.ts split) and TD-032 (N+1 auth queries).

---

### TD-NEW-2: `push_analyses` Lacks Explicit Retention Policy

- **Suggested ID:** TD-052
- **Severity:** LOW
- **Source:** DB-AUDIT.md Section 5.3
- **Description:** While TD-027 mentions `push_analyses` in passing, it does not provide a specific retention strategy. Each `push_analyses` row contains a `result_json` JSONB column with the full AI analysis result, which can be moderately large. Without pruning, this table grows indefinitely.
- **Remediation:** Add to the retention policy job: keep push analyses for 180 days (free) / 365 days (pro). Alternatively, archive `result_json` to cold storage after 90 days and keep only the summary columns.
- **Effort:** S (< 2h) -- part of TD-027 implementation.

---

### TD-NEW-3: `battle_history.created_at` Is Nullable

- **Suggested ID:** TD-053
- **Severity:** LOW
- **Source:** DB-AUDIT.md Section 3.3
- **Description:** `battle_history.created_at` has a `defaultNow()` but is nullable (verified at `schema.ts:245`). A buggy INSERT that explicitly passes `null` for `created_at` would bypass the default. Since `created_at` is used for pruning logic, a NULL value could cause the row to be either always or never pruned, depending on the comparison behavior.
- **Remediation:** Add `NOT NULL` constraint to `battle_history.created_at`. Verify no existing rows have NULL values first.
- **Effort:** S (< 2h).
- **Dependencies:** TD-022 (versioned migrations).

---

## 4. Dependency Validation

### Are DB-Related Dependencies Correctly Mapped?

**Mostly yes.** The DRAFT correctly identifies TD-022 (versioned migrations) as the foundational dependency for schema changes. The dependency chain is:

```
TD-022 (migrations) ──> TD-015 (subscriptions unique)
                   ──> TD-012 (notification consolidation)
                   ──> TD-026 (CHECK constraints)
                   ──> TD-036 (timestamptz)
                   ──> TD-041 (deprecate clash_tag)
                   ──> TD-048 (index naming)
```

**Correction:** The DRAFT shows TD-025 (bootstrap refactor) as having no dependencies, which is correct. However, it should be noted that TD-025 is a *prerequisite* for TD-012 -- refactoring bootstrap first makes the notification consolidation cleaner because there are fewer places to update.

**Missing dependency:** TD-015 (subscriptions unique) has an implicit dependency on TD-019 (pool config). If you add a unique constraint and the data cleanup runs via a long-running query on a pool with unconfigured connection limits, you risk connection exhaustion during the migration. TD-019 should be done first (which the roadmap already achieves by putting TD-019 in Phase 1).

### Suggested Execution Order for DB Items

```
Phase 1 (Quick Wins):
  1. TD-013 -- Add updated_at trigger (no dependencies, highest DB impact)
  2. TD-014 -- Add missing composite indexes (no dependencies, performance win)
  3. TD-019 -- Configure connection pool (no dependencies, infrastructure hardening)
  4. TD-049 -- Delete dead Replit script (trivial cleanup)

Phase 2 (Structural):
  5. TD-022 -- Introduce versioned migrations (unblocks all schema changes)
  6. TD-025 -- Refactor bootstrapUserData (unblocks TD-012)

Phase 3 (Integrity -- I recommend moving this BEFORE the i18n phase):
  7. TD-015 -- Add unique constraint on subscriptions.user_id (data cleanup first)
  8. TD-012 -- Consolidate notification preferences (after TD-025)
  9. TD-026 -- Add CHECK constraints (after TD-022)
  10. TD-027 -- Implement data retention policies (standalone)

Phase 4 (Optimization):
  11. TD-032 -- Consolidate N+1 auth queries
  12. TD-033 -- Fix favorite_players staleness
  13. TD-051 -- runAsUser session pattern (after TD-032)

Phase 5 (Polish):
  14. TD-036 -- Migrate to timestamptz (low priority)
  15. TD-041 -- Deprecate clash_tag column
  16. TD-048 -- Standardize index naming
  17. TD-052 -- push_analyses retention (part of TD-027)
  18. TD-053 -- battle_history.created_at NOT NULL
```

---

## 5. Specialist Recommendations

### Quick Wins (High Impact, Low Effort)

| Priority | Item | Effort | Impact |
|:--------:|------|:------:|--------|
| 1 | TD-013: `updated_at` trigger | 1-2h | Prevents clock skew and missed timestamp updates across all 9 tables. Every future update benefits. |
| 2 | TD-014: Missing composite indexes | 1h | Immediate query performance improvement for rate-limiting queries executed on every coach/push request. |
| 3 | TD-019: Connection pool configuration | 30min | Prevents connection exhaustion under moderate load. Critical for Vercel serverless. |
| 4 | TD-026: CHECK constraints | 1-2h | Defense in depth against data corruption. Simple ALTER TABLE statements. |

### Migration Strategy Recommendations

1. **Adopt a dual-track migration approach:**
   - **Track 1 (Drizzle):** Use `drizzle-kit generate` + `drizzle-kit migrate` for all schema changes managed by the ORM (tables, columns, indexes, constraints).
   - **Track 2 (SQL scripts):** Keep `rls-and-triggers.sql` and `decks-migrations.sql` for Supabase-specific features (RLS policies, triggers, grants, functions). Run these via `npm run supabase:apply` as today.
   - Both tracks should be versioned in git. Drizzle tracks its own migration history; SQL scripts are idempotent by design.

2. **Use `DATABASE_MIGRATIONS_URL` for direct connections:**
   - Migrations need a direct (non-pooled) connection to run DDL statements.
   - The app should use the pooled connection URL (`?pgbouncer=true` or Supabase's pooler endpoint).
   - `drizzle.config.ts` already supports this pattern (`DATABASE_MIGRATIONS_URL || DATABASE_URL`).

3. **Before each schema migration, run a pre-check script:**
   - Verify the migration SQL against a local/staging database.
   - For constraint additions (UNIQUE, CHECK), run a query to check for existing violations.
   - For column drops, verify no application code references the column.

### Monitoring/Alerting Suggestions

1. **Table size monitoring:** Set up a weekly check for table row counts, especially `coach_messages`, `battle_history`, `push_analyses`, and `deck_suggestions_usage`. Alert if any table exceeds 1M rows without a retention policy in place.

2. **Connection pool saturation:** Log `pool.totalCount`, `pool.idleCount`, and `pool.waitingCount` at startup and periodically. Alert if `waitingCount > 0` persists for more than 30 seconds.

3. **Query latency:** Instrument the `runAsUser` method to log execution time. Alert if any query consistently exceeds 500ms.

4. **Orphan data detection:** After TD-015 (subscriptions unique constraint), add a weekly check for other potential orphan patterns (e.g., `notification_preferences` rows without a matching `user_settings` row, or vice versa).

### Backup/Recovery Considerations

1. **Before TD-012 (notification consolidation) and TD-015 (subscriptions cleanup):** Take a full database backup. Both operations involve data deletion/consolidation that cannot be undone without a backup.

2. **Supabase Point-in-Time Recovery (PITR):** Verify that PITR is enabled on the Supabase project. For the Pro tier, Supabase provides 7-day PITR. This is critical before running any destructive migration.

3. **Migration rollback strategy:** Since Drizzle does not natively support down-migrations, each migration should include a commented-out rollback SQL block at the top of the file. This provides a manual rollback path if needed.

4. **`battle_history` JSONB backup:** Before implementing retention policies (TD-027), consider whether old battle data has analytical value. If so, export to a cold storage bucket (e.g., Supabase Storage or S3) before deletion.

---

## 6. Verdict

### VALIDATED WITH CHANGES

The Technical Debt DRAFT is a thorough and well-organized assessment of the CRStats database debt. The item descriptions are technically accurate, the dependency mapping is mostly correct, and the remediation approaches are sound. However, the following changes are required before finalization:

### Required Changes

| # | Change | Severity |
|---|--------|:--------:|
| 1 | **Restore TD-012 to CRITICAL severity.** The downgrade from CRITICAL (DB Audit) to HIGH is not justified. The dual-table fallback chain with the `settingsUpdateInputSchema` accepting conflicting values makes this an active consistency hazard, not a theoretical risk. | HIGH |
| 2 | **Restore TD-013 to CRITICAL severity.** 14 manual `updatedAt: new Date()` calls across the codebase with no trigger safety net is a CRITICAL gap. Clock skew between serverless functions and PostgreSQL is real, not theoretical. | HIGH |
| 3 | **Re-estimate TD-012 effort from L to XL.** 7 change points across 4 files (schema, storage, routes, SQL trigger) plus production data migration. | MEDIUM |
| 4 | **Re-estimate TD-015 effort from M to L.** Data cleanup in production is the bottleneck; the code change is straightforward. | MEDIUM |
| 5 | **Correct TD-032 description.** The 5-query scenario only applies when `bootstrapUserData()` is NOT called. When bootstrap runs (first login), it already returns all 4 entities. Clarify the two code paths. | LOW |
| 6 | **Add TD-051 (runAsUser transaction overhead).** This MEDIUM-severity item was identified in DB-AUDIT Section 7.2 but omitted from the DRAFT. | MEDIUM |
| 7 | **Add TD-052 and TD-053** as LOW-severity items (push_analyses retention, battle_history.created_at NOT NULL). These can be rolled into existing items (TD-027 and a general "nullability fixes" item). | LOW |
| 8 | **Move database integrity fixes (TD-012, TD-015) earlier in the roadmap.** The current placement in Phase 4 (Week 4-5) puts data integrity after i18n fixes (Phase 3). Data integrity should take priority -- these are foundational issues that get worse with every new user. I recommend swapping Phases 3 and 4, or at minimum pulling TD-015 and TD-012 into early Phase 3. | MEDIUM |
| 9 | **Downgrade TD-036 (timestamptz) from MEDIUM to LOW.** The practical risk is near-zero in a UTC-only, Supabase-hosted environment. Move to Phase 5. | LOW |

### Advisory Changes (Recommended but Not Blocking)

| # | Change |
|---|--------|
| A | Add migration strategy guidance (dual-track Drizzle + SQL scripts) to the remediation roadmap preamble. |
| B | Add backup/recovery checklist for TD-012 and TD-015 as pre-conditions in the roadmap. |
| C | Note in TD-019 that the pooler URL should be used for the app and the direct URL for migrations only. |
| D | Add TD-025 as an explicit prerequisite for TD-012 (bootstrap refactor before notification consolidation). |

---

*Review generated by @data-engineer (Dara) for Brownfield Discovery Phase 5.*
